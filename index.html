---
layout: default
tags: about
---
<!DOCTYPE html>
<html>
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-9NY2CVV92L"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-9NY2CVV92L');
  </script>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
<script type="text/javascript">
    function readMore() {
        $('#readMore').hide();
        $('#more').show ();
    }

    function readLess() {
        $('#readMore').show();
        $('#more').hide();
    }
</script>

<style>
*:not(a) {
  color: #000000 !important;
}
</style>

<style>
  p.normal {font-style:normal;}
  p.italic {font-style:italic; color:darkred}
  p.oblique {font-style:oblique;}
</style>

</head>

<body>

<img src="images/me.png" alt="Kaituo Feng" width="360" style="float: right; padding: 5px 0 0 15px; border-radius: 0%;" />


<div class="bio" style="text-align:justify">
      <p>
         I am a first-year PhD student at <a href="https://mmlab.ie.cuhk.edu.hk/" style="color: blue;" class="uline">Multimedia Lab (MMLab) </a>in the Chinese University of Hong Kong, working with <a href="https://xyue.io/" style="color: blue;" class="uline">Prof. Xiangyu Yue</a>. Previously, I was a master student at Beijing Institute of Technology (BIT), advised by <a href="https://cs.bit.edu.cn/szdw/jsml/gjjgccrc/29786fd2fa80455b9a5922066ed96a4a.htm" style="color: blue;" class="uline">Prof. Changsheng Li</a> (2022-2025). I also received my Bachelor's degree in Computer Science from <a href="https://www.bit.edu.cn" style="color: blue;" class="uline">BIT</a> (2018 - 2022). I have published several papers in top conferences or journals, such as ICML, ICLR, CVPR, KDD, IEEE TIP, IEEE TKDE, IEEE TPAMI, etc.  
      </p>

      <p>
        My research interests include MLLMs and AIGC. Welcome for discussion and collaboration, feel free to drop me an email.
      </p>
        <p>
          Email: kaituofeng[at]gmail[dot]com
        </p>
        <p>
          [<a href="https://scholar.google.com/citations?user=m1iCh00AAAAJ&hl=en" class="uline">Google Scholar</a>]  [<a href="https://github.com/tulerfeng" class="uline">Github</a>] 
      </p>
        <br/>
</div> 

<!-- <hr />
<div class="news">
    <h2>News</h2>
    <br />
    <ul>
      <li>
        <p>[July 2022] One paper was accepted by ACM Multimedia (ACM MM) 2022!</p>
      </li>
        <li>
          <p>[Apr 2022] <a href='https://ieeexplore.ieee.org/document/9756672' class='uline'>Critical Classes and Samples Discovering for Partial Domain Adaptation</a> was accepted to IEEE Transaction on Cybernetics (IF: 11.44)!</p>
        </li>
        <li>
            <p>[July 2021] <a href='https://proceedings.neurips.cc/paper/2021/file/6ba3af5d7b2790e73f0de32e5c8c1798-Paper.pdf' class='uline'>Pareto Domain Adaptation </a> was accepted to NeurIPS 2021! </p>
          </li>
          <li>
              <p>[Apr 2021] Two papers (One <font color="red">Oral</font>) were accepted to CVPR 2021!</p>
          </li>
          
    </ul>
</div>

<hr /> -->

<div id="research">
    <h2><a name="research">Selected Publications</a></h2>
    <br />
    <table width="100%" align="center" cellspacing="0" cellpadding="0" style="border-collapse: collapse;">
        <tbody>

            <!-- arxiv video -->
          <tr>
              <td valign="middle" width="35%">
                  <div class="one" style="text-align:center;">
                      <img src="images/videor1.png" style="max-height: 300px;">
                  </div>
              </td>
              <td valign="middle" width="65%">
                  <h5>
                    Video-R1: Reinforcing Video Reasoning in MLLMs
                  </h5>
                  <p>
                      <a href="" class="uline-special"><span style="color:red">arXiv 2025</span></a>
                  </p>
                  <p>
                      <b>Kaituo Feng</b>, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, Xiangyu Yue
                  </p>
                  <p>
                    Explore the R1 paradigm for eliciting video reasoning within MLLMs.
                  </p>
                  <a href="https://arxiv.org/abs/2503.21776" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Paper</a>
                  <a href="https://github.com/tulerfeng/Video-R1" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Code</a>
              </td>
          </tr>
          
          <!-- arxiv spatial -->
          <tr>
              <td valign="middle" width="35%">
                  <div class="one" style="text-align:center;">
                      <img src="images/spatial.png" style="max-height: 300px;">
                  </div>
              </td>
              <td valign="middle" width="65%">
                  <h5>
                    Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing
                  </h5>
                  <p>
                      <a href="" class="uline-special"><span style="color:red">arXiv 2025</span></a>
                  </p>
                  <p>
                      Junfei Wu, Jian Guan, <b>Kaituo Feng</b>, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, Tieniu Tan
                  </p>
                  <p>
                    Achieveing o3-like thinking for spatial reasoning across images and videos.
                  </p>
                  <a href="https://arxiv.org/abs/2506.09965" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Paper</a>
                  <a href="https://github.com/AntResearchNLP/ViLaSR" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Code</a>
              </td>
          </tr>
          

          <!-- arxiv sophiavl-r1 -->
          <tr>
              <td valign="middle" width="35%">
                  <div class="one" style="text-align:center;">
                      <img src="images/sophia.png" style="max-height: 300px;">
                  </div>
              </td>
              <td valign="middle" width="65%">
                  <h5>
                    SophiaVL-R1: Reinforcing MLLMs Reasoning with Thinking Reward
                  </h5>
                  <p>
                      <a href="" class="uline-special"><span style="color:red">arXiv 2025</span></a>
                  </p>
                  <p>
                      Kaixuan Fan*, <b>Kaituo Feng*</b>, Haoming Lyu, Dongzhan Zhou, Xiangyu Yue (*equal contribution)
                  </p>
                  <p>
                    Intergrating thinking-level reward to address the phenomenon of "wrong thinking, correct answer".
                  </p>
                  <a href="https://arxiv.org/abs/2505.17018" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Paper</a>
                  <a href="https://github.com/kxfan2002/SophiaVL-R1" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Code</a>
              </td>
          </tr>

          
          <!-- arxiv critique -->
          <tr>
              <td valign="middle" width="35%">
                  <div class="one" style="text-align:center;">
                      <img src="images/critique.png" style="max-height: 300px;">
                  </div>
              </td>
              <td valign="middle" width="65%">
                  <h5>
                    Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback
                  </h5>
                  <p>
                      <a href="" class="uline-special"><span style="color:red">arXiv 2025</span></a>
                  </p>
                  <p>
                      Xiaoying Zhang, Hao Sun, Yipeng Zhang, <b>Kaituo Feng</b>, Chaochao Lu, Chao Yang, Helen Meng
                  </p>
                  <p>
                    Using external critiques as language feedback for improving reasoning
                  </p>
                  <a href="https://arxiv.org/abs/2506.03106" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Paper</a>
                  <a href="https://github.com/zhangxy-2019/critique-GRPO1" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Code</a>
              </td>
          </tr>


          <!-- arxiv avodyssey -->
          <tr>
              <td valign="middle" width="35%">
                  <div class="one" style="text-align:center;">
                      <img src="images/avo.png" style="max-height: 300px;">
                  </div>
              </td>
              <td valign="middle" width="65%">
                  <h5>
                    AV-Odyssey: Can Your Multimodal LLMs Really Understand Audio-Visual Information?
                  </h5>
                  <p>
                      <a href="" class="uline-special"><span style="color:red">arXiv 2024</span></a>
                  </p>
                  <p>
                      Kaixiong Gong*, <b>Kaituo Feng*</b>, Bohao Li*, Yibing Wang, Mofan Cheng, Shijia Yang, Jiaming Han, Benyou Wang, Yutong Bai, Zhuoran Yang, Xiangyu Yue (*equal contribution)
                  </p>
                  <p>
                    We propose a comprehensive benchmark for evaluating audio-visual understanding abilities of MLLMs. 
                  </p>
                  <a href="https://arxiv.org/pdf/2412.02611" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Paper</a>
                  <a href="https://github.com/AV-Odyssey/AV-Odyssey" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Code</a>
              </td>
          </tr>


          <!-- CVPR PlanKD -->
          <tr>
              <td valign="middle" width="35%">
                  <div class="one" style="text-align:center;">
                      <img src="images/plankd.png" style="max-height: 300px;">
                  </div>
              </td>
              <td valign="middle" width="65%">
                  <h5>
                    On the Road to Portability: Compressing End-to-End Motion Planner for Autonomous Driving
                  </h5>
                  <p>
                      <a href="https://cvpr.thecvf.com/Conferences/2024" class="uline-special"><span style="color:red">CVPR 2024</span></a>
                  </p>
                  <p>
                      <b>Kaituo Feng</b>, Changsheng Li, Dongchun Ren, Ye Yuan, Guoren Wang
                  </p>
                  <p>
                    We constitute the first attempt to explore a knowledge distillation method to compress end-to-end autonomous driving planners.
                  </p>
                  <a href="https://arxiv.org/abs/2403.01238" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Paper</a>
                  <a href="https://github.com/tulerfeng/PlanKD" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Code</a>
              </td>
          </tr>


          <!-- ICML KPOD -->
          <tr>
            <td valign="middle" width="35%">
                <div class="one" style="text-align:center;">
                    <img src="images/kpod.png" style="max-height: 300px;">
                </div>
            </td>
            <td valign="middle" width="65%">
                <h5>
                  Keypoint-based Progressive Chain-of-Thought Distillation for LLMs
                </h5>
                <p>
                    <a href="https://icml.cc/Conferences/2024" class="uline-special"><span style="color:red">ICML 2024</span></a>
                </p>
                <p>
                    <b>Kaituo Feng</b>, Changsheng Li, Xiaolu Zhang, Jun Zhou, Ye Yuan, Guoren Wang
                </p>
                <p>
                  We propose a new compression method to progressively distill the emergent reasoning capabilities of LLMs into smaller models, as well as encouraging the precise mimicry of significant tokens.
                </p>
                <a href="https://arxiv.org/abs/2405.16064" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Paper</a>
            </td>
        </tr>

           
          <!-- ICLR OTGNet -->
          <tr>
            <td valign="middle" width="35%">
                <div class="one" style="text-align:center;">
                    <img src="images/otgnet.png" style="max-height: 300px;">
                </div>
            </td>
            <td valign="middle" width="65%">
                <h5>
                  Towards Open Temporal Graph Neural Networks
                </h5>
                <p>
                    <a href="https://iclr.cc/Conferences/2023" class="uline-special"><span style="color:red">ICLR 2023, Oral, 90/4922</span></a>
                </p>
                <p>
                    <b>Kaituo Feng</b>, Changsheng Li, Xiaolu Zhang, Jun Zhou
                </p>
                <p>
                  We propose the first class-incremental learning for temporal GNNs, allowing temporal graphs to evolve in the real-world scenarios with an open class set
                </p>
                <a href="https://arxiv.org/abs/2303.15015" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Paper</a>
                <a href="https://github.com/tulerfeng/OTGNet" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Code</a>
            </td>
        </tr>



        </tbody>
    </table>
</div>

<hr>




<!-- <div class="news">
  <h2>Academic Service</h2>
  <br />
  <p>
    Serve as a reviewer for IEEE CVPR 2021-2022, AAAI 2021-2022, IEEE ICCV 2021, ECCV 2022 etc.
  </p>
  
</div> -->




<!-- <div class="news">
  <h2>Course</h2>
  <br />
  <p>
    Introduction to Machine Learning, since 2021, Instructor: <a href="https://shuangli.xyz" class="uline">Prof. Li</a> 
  </p>
  
</div> -->


<hr>
<div id="news">
  <h2>Selected Honors and Awards</h2>
  <ul>
    <li>
      <p>
        National Scholarship, Ministry of Education of China (TOP 2%), 2024.
      </p>
    </li>
    <li>
      <p>
        National Scholarship, Ministry of Education of China (TOP 2%), 2023.
      </p>
    </li>
    <li>
      <p>
        Outstanding Undergraduate Student of Beijing Institute of Technology, 2022.
      </p>
    </li>
    <li>
      <p>
        Silver Medal of 45th ACM-ICPC Asia Regional Contest, 2020.
      </p>
    </li>
    <li>
      <p>
        First Prize (top 1%) of China Undergraduate Mathematical Contest in Modeling (CUMCM), 2020.
      </p>
    </li>
    <li>
      <p>
        Gold Medal of Group Programming Ladder Tournament China Finals, 2020.
      </p>
    </li>
  </ul>
</div>

<hr>
<div class="news">
  <h2>Contact</h2>
  <br />
  <ul>
    <li>
        <p>
          Email: kaituofeng@gmail.com
        </p>
    </li>
</ul>

</div>




<!-- <div id="projects">
  <h2>Projects</h2>
  <table width="100%" align="center" cellspacing="0" cellpadding="0" style="border-collapse: collapse;">
    <tbody>
      <tr>
        <td valign="top" width="40%">
          <h5>PyTorch implementation of <a href="https://arxiv.org/" class="uline"> Deep Reinforcement Learning</a></h5>
          <p class="authors">
            <b>Binhui Xie</b>, 
          </p>
          <a href="https://github.com/" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Code</a>
        </td>
        <td width="50%">
          <div class="one" style="text-align:center;">
            <div class="two" id="jump_image" style="opacity: 0;"></div>
            <img src="images/drl.jpg" style="margin-top:30px;max-height: 200px;width:100%;">
          </div>
        </td>
      </tr>
    </tbody>
  </table>
</div> -->
        <!-- <hr> -->

</body>
<script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=5n0qvv7rnap&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33" async="async"></script>
</html>
